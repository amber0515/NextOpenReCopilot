"""
Remote Model - OpenAI/Anthropic API Client

This module provides backward-compatible interface to LLM APIs.
Internally delegates to infrastructure.llm_api for actual API calls.
"""

import asyncio
from infrastructure.llm_api import LLMApiFactory, LLMRequest, LLMApi

# Configuration and templates
from config import settings_manager, PROMPT_TEMPLATE
from task_guides import TASK_GUIDES, TASK_OUTPUT_FORMATS, get_mock_response


class OpenAIModel:
    """
    OpenAI/Anthropic Model client with backward-compatible interface.

    This class acts as a facade/adapter that:
    1. Formats prompts using templates and task guides
    2. Delegates actual API calls to infrastructure.llm_api
    3. Maintains the same return format: (response_text, prompt_for_feedback)
    """

    def __init__(self):
        self._api: LLMApi = None
        self._cancelled = False

    def _refresh_api(self):
        """Refresh API instance based on current settings."""
        if self._api is None:
            self._api = LLMApiFactory.create_from_config(settings_manager.settings)

    def cancel(self):
        """å–æ¶ˆå½“å‰æ­£åœ¨è¿›è¡Œçš„æ¨¡å‹è°ƒç”¨ã€‚"""
        self._cancelled = True
        if self._api:
            self._api.cancel()

    def _reset_cancel_state(self):
        """é‡ç½®å–æ¶ˆæ ‡å¿—"""
        self._cancelled = False

    async def call_model(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥è°ƒç”¨ OpenAI æ¨¡å‹ã€‚

        Args:
            prompt: ç”¨æˆ·æä¾›çš„æ ¸å¿ƒæç¤ºå†…å®¹ã€‚
            task_tag: ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºé€‰æ‹©ä»»åŠ¡æŒ‡å—å’Œè¾“å‡ºæ ¼å¼ã€‚
            timeout: API è°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚

        Returns:
            ä¸€ä¸ªå…ƒç»„ (model_response_text, prompt_for_feedback)ã€‚
        """
        self._reset_cancel_state()

        # 1. æ ¼å¼åŒ– Prompt
        formatted_prompt, prompt_for_feedback = self._format_prompt(prompt, task_tag)

        # 2. è°ƒè¯•è¾“å‡º
        self._debug_output(formatted_prompt, task_tag, timeout)

        # 3. è·å– API å®ä¾‹å¹¶è°ƒç”¨
        self._refresh_api()

        # 4. æ„å»º LLMRequest
        request = LLMRequest(
            prompt=formatted_prompt,
            task_tag=task_tag,
            max_tokens=settings_manager.settings.get('max_output_tokens', 2048),
            temperature=0.6,
            timeout=timeout
        )

        # 5. æµå¼è°ƒç”¨å¹¶æ”¶é›†å“åº”
        response_parts = []
        async for chunk in self._api.call_stream(request):
            if self._cancelled and chunk.startswith("<Cancelled>"):
                print("\n[!ğŸ’¥] Analysis cancelled by user")
                return chunk, prompt_for_feedback
            response_parts.append(chunk)

        print()  # æµç»“æŸåæ¢è¡Œ
        final_response = "".join(response_parts)
        return final_response, prompt_for_feedback

    async def _call_anthropic_model(self, formatted_prompt: str, model_name: str,
                                     api_key: str, base_url: str, timeout: int, prompt_for_feedback: str):
        """
        ä¿ç•™ç”¨äºå‘åå…¼å®¹ã€‚ç°åœ¨å§”æ‰˜ç»™ç»Ÿä¸€çš„ API è°ƒç”¨ã€‚
        """
        from infrastructure.llm_api import AnthropicApi

        api = AnthropicApi(base_url, api_key, model_name)
        request = LLMRequest(
            prompt=formatted_prompt,
            task_tag="",  # Not used in direct call
            max_tokens=settings_manager.settings.get('max_output_tokens', 2048),
            timeout=timeout
        )

        response_parts = []
        async for chunk in api.call_stream(request):
            response_parts.append(chunk)

        return "".join(response_parts), prompt_for_feedback

    async def call_model_mock(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥æ¨¡æ‹Ÿè°ƒç”¨AIæ¨¡å‹ï¼Œç”¨äºè°ƒè¯•ã€‚
        """
        self._reset_cancel_state()

        # æ ¼å¼åŒ– Prompt
        formatted_prompt, prompt_for_feedback = self._format_prompt(prompt, task_tag)

        # è°ƒè¯•è¾“å‡º
        if settings_manager.settings.get('debug_mode', False):
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))

            base_url_setting = settings_manager.settings.get('base_url', 'N/A')
            api_key_setting = settings_manager.settings.get('api_key', 'N/A')[:5] + "..."
            model_name_setting = settings_manager.settings.get('model_name', 'mock_model')
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: base_url={base_url_setting}, api_key={api_key_setting}, model_name={model_name_setting}, timeout={timeout}s")
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: recv {len(formatted_prompt)} chars prompt")

        mock_response_full = get_mock_response(task_tag)

        print(f"[DEBUGğŸ›] Mock response for {task_tag}:")
        response_parts = []
        for line in mock_response_full.split('\n'):
            if self._cancelled:
                print("\n[!ğŸ’¥] Analysis cancelled by user (during mock streaming)")
                return "<Cancelled>Analysis cancelled by user", prompt_for_feedback

            print(f"[DEBUGğŸ›] {line}")
            response_parts.append(line)
            await asyncio.sleep(0.1)

        return "\n".join(response_parts), prompt_for_feedback

    def _format_prompt(self, prompt: str, task_tag: str):
        """
        æ ¼å¼åŒ– promptï¼Œä½¿ç”¨æ¨¡æ¿å’Œä»»åŠ¡æŒ‡å—ã€‚

        Returns:
            (formatted_prompt, prompt_for_feedback)
        """
        template_name = settings_manager.settings.get('prompt_template', 'recopilot')
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}")

        if template_name.endswith("_wo_guide"):
            formatted = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                         .replace("{input}", prompt)
        else:
            formatted = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                         .replace("{guide}", TASK_GUIDES.get(task_tag, "")) \
                                         .replace("{input}", prompt)

        return formatted, formatted

    def _debug_output(self, formatted_prompt: str, task_tag: str, timeout: int):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if settings_manager.settings.get('debug_mode', False):
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))

            model_name_setting = settings_manager.settings.get('model_name', 'unknown_model')
            print(f"[ğŸ”—] OpenAIModel.call_model: model_name={model_name_setting}, timeout={timeout}s")
            print(f"[ğŸ”—] OpenAIModel.call_model: send {len(formatted_prompt)} chars prompt")

        # æ€»æ˜¯æ‰“å°æ­£åœ¨è°ƒç”¨çš„æ¨¡å‹
        base_url = settings_manager.settings.get('base_url', '')
        model_name = settings_manager.settings.get('model_name', 'unknown')
        print(f"[ğŸ”—] Calling model: {model_name} at {base_url or 'OpenAI default'}")


# å…¨å±€å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
model = OpenAIModel()
