import openai # pip install openai
import anthropic # pip install anthropic
import time
import asyncio
import traceback # ç”¨äºæ‰“å°å¼‚å¸¸å †æ ˆ

# å‡è®¾è¿™äº›è‡ªå®šä¹‰æ¨¡å—å­˜åœ¨äºé¡¹ç›®ä¸­
from config import settings_manager, PROMPT_TEMPLATE # PROMPT_TEMPLATE å¯èƒ½æ˜¯å­—å…¸
from task_guides import TASK_GUIDES, TASK_OUTPUT_FORMATS, get_mock_response

# PyArmor ç›¸å…³çš„ __assert_armored__, __pyarmor_enter_XXXX__, __pyarmor_exit_XXXX__ è°ƒç”¨å·²çœç•¥

class OpenAIModel:
    def __init__(self):
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        super().__init__() # è™½ç„¶å­—èŠ‚ç ä¸­æ²¡æœ‰æ˜ç¡®çš„çˆ¶ç±»ï¼Œä½† super() è°ƒç”¨é€šå¸¸å­˜åœ¨
        self._current_completion = None # ç”¨äºå­˜å‚¨æ´»åŠ¨çš„æµå¼ API è°ƒç”¨å¯¹è±¡
        self._cancelled = False         # å–æ¶ˆæ ‡å¿—

    def _is_anthropic_api(self, base_url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º Anthropic APIã€‚"""
        return 'anthropic' in base_url.lower() if base_url else False

    def cancel(self):
        """å–æ¶ˆå½“å‰æ­£åœ¨è¿›è¡Œçš„æ¨¡å‹è°ƒç”¨ã€‚"""
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = True

    async def call_model(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥è°ƒç”¨ OpenAI æ¨¡å‹ã€‚

        Args:
            prompt: ç”¨æˆ·æä¾›çš„æ ¸å¿ƒæç¤ºå†…å®¹ã€‚
            task_tag: ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºé€‰æ‹©ä»»åŠ¡æŒ‡å—å’Œè¾“å‡ºæ ¼å¼ã€‚
            timeout: API è°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚

        Returns:
            ä¸€ä¸ªå…ƒç»„ (model_response_text, original_prompt_for_feedback)ã€‚
            å¦‚æœå‘ç”Ÿé”™è¯¯æˆ–å–æ¶ˆï¼Œmodel_response_text ä¼šåŒ…å«é”™è¯¯æˆ–å–æ¶ˆä¿¡æ¯ã€‚
        """
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = False # é‡ç½®å–æ¶ˆæ ‡å¿—
        
        # 1. è·å–å¹¶æ ¼å¼åŒ–æç¤ºæ¨¡æ¿
        template_name = settings_manager.settings.get('prompt_template', 'default_template_name') # ä»é…ç½®è·å–æ¨¡æ¿åç§°
        # å‡è®¾ PROMPT_TEMPLATE æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¾‹å¦‚:
        # PROMPT_TEMPLATE = {
        #     "default_template_name": "Format: {format}\nGuide: {guide}\nInput: {input}",
        #     "default_template_name_wo_guide": "Format: {format}\nInput: {input}"
        # }
        
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}") # è·å–æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œæä¾›é»˜è®¤å€¼

        if template_name.endswith("_wo_guide"): # å¦‚æœæ¨¡æ¿åç§°æŒ‡ç¤ºä¸ä½¿ç”¨æŒ‡å—
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        else:
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{guide}", TASK_GUIDES.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        
        # ä¸ºåé¦ˆå‡†å¤‡çš„åŸå§‹æç¤ºï¼ˆå¯èƒ½å°±æ˜¯æ ¼å¼åŒ–åçš„æç¤ºï¼‰
        prompt_for_feedback = formatted_prompt 

        # 2. æ‰“å°è°ƒè¯•ä¿¡æ¯ (å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼)
        if settings_manager.settings.get('debug_mode', False):
            # å­—èŠ‚ç ä¸­å¯¹ prompt è¿›è¡Œäº† .split('\n') å’Œ .join('\n[DEBUGğŸ›] ') æ“ä½œ
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))

            model_name_setting = settings_manager.settings.get('model_name', 'unknown_model')
            print(f"[ğŸ”—] OpenAIModel.call_model: model_name={model_name_setting}, timeout={timeout}s")
            print(f"[ğŸ”—] OpenAIModel.call_model: send {len(formatted_prompt)} chars prompt")

        # 3. åˆå§‹åŒ– API å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ OpenAI å…¼å®¹å’Œ Anthropicï¼‰
        try:
            base_url = settings_manager.settings.get('base_url', '')
            api_key = settings_manager.settings.get('api_key', 'sk-none')
            model_name = settings_manager.settings.get('model_name', 'gpt-3.5-turbo')

            print(f"[ğŸ”—] Calling model: {model_name} at {base_url or 'OpenAI default'}")

            # æ£€æµ‹æ˜¯å¦ä¸º Anthropic API
            if self._is_anthropic_api(base_url):
                # ä½¿ç”¨ Anthropic å®¢æˆ·ç«¯
                return await self._call_anthropic_model(
                    formatted_prompt, model_name, api_key, base_url, timeout, prompt_for_feedback
                )
            
            # ä½¿ç”¨ OpenAI å…¼å®¹å®¢æˆ·ç«¯
            # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
            client_args = {'api_key': api_key}
            if base_url:
                # ç¡®ä¿ base_url æ ¼å¼æ­£ç¡®
                if not base_url.endswith('/v1') and not base_url.endswith('/v1/'):
                    if not base_url.endswith('/'):
                        base_url += '/v1'
                    else:
                        base_url += 'v1'
                client_args['base_url'] = base_url

            # ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
            client = openai.AsyncOpenAI(**client_args)

            # 4. æ„å»ºæ¶ˆæ¯å¹¶å‘é€è¯·æ±‚
            messages = [{"role": "user", "content": formatted_prompt}]

            self._current_completion = await client.chat.completions.create(
                model=model_name,
                temperature=0.6,
                stream=True,
                max_tokens=settings_manager.settings.get('max_output_tokens', 2048),
                messages=messages,
                timeout=float(timeout)
            )

            # 5. å¤„ç†æµå¼å“åº”
            reasoning_content_parts = []
            async for chunk in self._current_completion:
                if self._cancelled:
                    print("\n[!ğŸ’¥] Analysis cancelled by user (during streaming)")
                    await self._current_completion.close() # ç¡®ä¿å…³é—­æµ
                    self._current_completion = None
                    return "<Cancelled>Analysis cancelled by user", prompt_for_feedback
                
                # æ£€æŸ¥ chunk.choices[0].delta.content
                # å­—èŠ‚ç ä¸­æ£€æŸ¥äº† hasattr(chunk.choices[0].delta, 'reasoning_content')
                # å’Œ hasattr(chunk.choices[0].delta, 'content')
                # è¿™è¡¨æ˜æ¨¡å‹å¯èƒ½è¿”å›å¸¦æœ‰ 'reasoning_content' æˆ– 'content' çš„ delta
                
                chunk_content = None
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        chunk_content = delta.reasoning_content
                    elif hasattr(delta, 'content') and delta.content:
                         chunk_content = delta.content
                
                if chunk_content:
                    print(chunk_content, end="") # å®æ—¶æ‰“å°ï¼Œä¸æ¢è¡Œ
                    reasoning_content_parts.append(chunk_content)
            
            print() # åœ¨æµç»“æŸåæ¢è¡Œ
            self._current_completion = None
            final_response_text = "".join(reasoning_content_parts)
            return final_response_text, prompt_for_feedback

        except openai.APITimeoutError as e:
            print(f"[!ğŸ’¥] Error: OpenAI API request timed out: {e}")
            self._current_completion = None
            return f"<RequestException>Request timed out: {e}", prompt_for_feedback
        except openai.APIConnectionError as e:
            print(f"[!ğŸ’¥] Error: Failed to connect to API: {e}")
            self._current_completion = None
            return f"<RequestException>Connection failed: {e}", prompt_for_feedback
        except openai.AuthenticationError as e:
            print(f"[!ğŸ’¥] Error: API authentication failed: {e}")
            self._current_completion = None
            return f"<RequestException>Authentication failed - check your API key: {e}", prompt_for_feedback
        except Exception as e:
            print(f"[!ğŸ’¥] Error in OpenAIModel.call_model: {e}")
            traceback.print_exc()
            self._current_completion = None # æ¸…ç†
            return f"<RequestException>{str(e)}", prompt_for_feedback

    async def _call_anthropic_model(self, formatted_prompt: str, model_name: str, 
                                     api_key: str, base_url: str, timeout: int, prompt_for_feedback: str):
        """
        ä½¿ç”¨ Anthropic API è°ƒç”¨ Claude æ¨¡å‹ã€‚
        
        Args:
            formatted_prompt: æ ¼å¼åŒ–åçš„æç¤ºè¯
            model_name: æ¨¡å‹åç§° (å¦‚ claude-sonnet-4-20250514)
            api_key: Anthropic API å¯†é’¥
            base_url: API åŸºç¡€ URL (å¦‚ https://open.bigmodel.cn/api/anthropic)
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            prompt_for_feedback: ç”¨äºåé¦ˆçš„åŸå§‹æç¤ºè¯
            
        Returns:
            å…ƒç»„ (å“åº”æ–‡æœ¬, åŸå§‹æç¤ºè¯)
        """
        try:
            # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
            client_args = {'api_key': api_key}
            if base_url:
                client_args['base_url'] = base_url
            
            client = anthropic.AsyncAnthropic(**client_args)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": formatted_prompt}]
            
            # åˆ›å»ºæµå¼è¯·æ±‚
            reasoning_content_parts = []
            
            async with client.messages.stream(
                model=model_name,
                max_tokens=settings_manager.settings.get('max_output_tokens', 2048),
                messages=messages,
            ) as stream:
                self._current_completion = stream
                async for text in stream.text_stream:
                    if self._cancelled:
                        print("\n[!ğŸ’¥] Analysis cancelled by user (during Anthropic streaming)")
                        self._current_completion = None
                        return "<Cancelled>Analysis cancelled by user", prompt_for_feedback
                    
                    print(text, end="")
                    reasoning_content_parts.append(text)
            
            print()  # åœ¨æµç»“æŸåæ¢è¡Œ
            self._current_completion = None
            final_response_text = "".join(reasoning_content_parts)
            return final_response_text, prompt_for_feedback
            
        except anthropic.APIConnectionError as e:
            print(f"[!ğŸ’¥] Error: Failed to connect to Anthropic API: {e}")
            self._current_completion = None
            return f"<RequestException>Connection failed: {e}", prompt_for_feedback
        except anthropic.AuthenticationError as e:
            print(f"[!ğŸ’¥] Error: Anthropic API authentication failed: {e}")
            self._current_completion = None
            return f"<RequestException>Authentication failed - check your API key: {e}", prompt_for_feedback
        except anthropic.RateLimitError as e:
            print(f"[!ğŸ’¥] Error: Anthropic API rate limit exceeded: {e}")
            self._current_completion = None
            return f"<RequestException>Rate limit exceeded: {e}", prompt_for_feedback
        except Exception as e:
            print(f"[!ğŸ’¥] Error in Anthropic API call: {e}")
            traceback.print_exc()
            self._current_completion = None
            return f"<RequestException>{str(e)}", prompt_for_feedback

    async def call_model_mock(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥æ¨¡æ‹Ÿè°ƒç”¨AIæ¨¡å‹ï¼Œç”¨äºè°ƒè¯•ã€‚
        """
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = False
        
        template_name = settings_manager.settings.get('prompt_template', 'default_template_name')
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}")

        if template_name.endswith("_wo_guide"):
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        else:
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{guide}", TASK_GUIDES.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        
        prompt_for_feedback = formatted_prompt

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if settings_manager.settings.get('debug_mode', False):
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))
            
            base_url_setting = settings_manager.settings.get('base_url', 'N/A')
            api_key_setting = settings_manager.settings.get('api_key', 'N/A')[:5] + "..." # ä»…æ˜¾ç¤ºéƒ¨åˆ†APIå¯†é’¥
            model_name_setting = settings_manager.settings.get('model_name', 'mock_model')
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: base_url={base_url_setting}, api_key={api_key_setting}, model_name={model_name_setting}, timeout={timeout}s")
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: recv {len(formatted_prompt)} chars prompt")

        mock_response_full = get_mock_response(task_tag)
        
        print(f"[DEBUGğŸ›] Mock response for {task_tag}:")
        response_parts = []
        for line in mock_response_full.split('\n'): # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            if self._cancelled:
                print("\n[!ğŸ’¥] Analysis cancelled by user (during mock streaming)")
                return "<Cancelled>Analysis cancelled by user", prompt_for_feedback
            
            print(f"[DEBUGğŸ›] {line}") # é€è¡Œæ‰“å°æ¨¡æ‹Ÿå“åº”
            response_parts.append(line)
            await asyncio.sleep(0.1) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ

        return "\n".join(response_parts), prompt_for_feedback