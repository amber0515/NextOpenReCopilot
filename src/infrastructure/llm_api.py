"""
Infrastructure Layer - LLM API

This module provides a unified interface for LLM API calls,
supporting OpenAI-compatible and Anthropic Claude APIs.
Extracted and refactored from remote_model.py.

Following the architecture design in ARCHITECTURE_REFACTORING_PLAN.md
"""

import asyncio
import traceback
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import AsyncIterator, Optional


@dataclass
class LLMRequest:
    """LLM è¯·æ±‚"""
    prompt: str
    task_tag: str
    max_tokens: int = 2048
    temperature: float = 0.6
    timeout: int = 600


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    raw_prompt: str


class LLMApi(ABC):
    """LLM API æŠ½è±¡æ¥å£"""

    def __init__(self, base_url: str, api_key: str, model_name: str):
        self.base_url = base_url
        self.api_key = api_key
        self.model_name = model_name
        self._cancelled = False

    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        self._cancelled = True

    def _reset_cancel_state(self):
        """é‡ç½®å–æ¶ˆæ ‡å¿—"""
        self._cancelled = False

    def _is_cancelled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ"""
        return self._cancelled

    @abstractmethod
    async def call(self, request: LLMRequest) -> LLMResponse:
        """
        è°ƒç”¨ LLMï¼ˆè¿”å›å®Œæ•´å“åº”ï¼‰

        Args:
            request: LLM è¯·æ±‚

        Returns:
            LLM å“åº”
        """
        pass

    @abstractmethod
    async def call_stream(self, request: LLMRequest) -> AsyncIterator[str]:
        """
        æµå¼è°ƒç”¨ LLMï¼ˆè¿”å›è¿­ä»£å™¨ï¼‰

        Args:
            request: LLM è¯·æ±‚

        Yields:
            å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        pass


class OpenAIApi(LLMApi):
    """
    OpenAI å…¼å®¹ API å®ç°

    æ”¯æŒï¼š
    - OpenAI å®˜æ–¹ API
    - DeepSeek (https://api.deepseek.com)
    - Ollama æœ¬åœ° API (http://localhost:11434)
    - å…¶ä»– OpenAI å…¼å®¹çš„ API
    """

    async def call(self, request: LLMRequest) -> LLMResponse:
        """
        è°ƒç”¨ OpenAI å…¼å®¹ APIï¼Œè¿”å›å®Œæ•´å“åº”
        """
        self._reset_cancel_state()

        try:
            import openai
        except ImportError:
            raise ImportError("openai package is required. Install with: pip install openai")

        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_args = {'api_key': self.api_key or 'sk-none'}
        if self.base_url:
            client_args['base_url'] = self.base_url

        client = openai.AsyncOpenAI(**client_args)

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": request.prompt}]

        try:
            # å‘é€è¯·æ±‚ï¼ˆéæµå¼ï¼‰
            response = await client.chat.completions.create(
                model=self.model_name,
                temperature=request.temperature,
                stream=False,
                max_tokens=request.max_tokens,
                messages=messages,
                timeout=float(request.timeout)
            )

            content = response.choices[0].message.content
            return LLMResponse(content=content, raw_prompt=request.prompt)

        except openai.APITimeoutError as e:
            error_msg = f"<RequestException>Request timed out: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except openai.APIConnectionError as e:
            error_msg = f"<RequestException>Connection failed: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except openai.AuthenticationError as e:
            error_msg = f"<RequestException>Authentication failed - check your API key: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except Exception as e:
            error_msg = f"<RequestException>{str(e)}"
            traceback.print_exc()
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)

    async def call_stream(self, request: LLMRequest) -> AsyncIterator[str]:
        """
        æµå¼è°ƒç”¨ OpenAI å…¼å®¹ API

        å®æ—¶æ‰“å°å“åº”å†…å®¹ï¼Œå¹¶é€å—è¿”å›
        """
        self._reset_cancel_state()

        try:
            import openai
        except ImportError:
            raise ImportError("openai package is required. Install with: pip install openai")

        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_args = {'api_key': self.api_key or 'sk-none'}
        if self.base_url:
            client_args['base_url'] = self.base_url

        client = openai.AsyncOpenAI(**client_args)

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": request.prompt}]

        try:
            # å‘é€æµå¼è¯·æ±‚
            completion = await client.chat.completions.create(
                model=self.model_name,
                temperature=request.temperature,
                stream=True,
                max_tokens=request.max_tokens,
                messages=messages,
                timeout=float(request.timeout)
            )

            async for chunk in completion:
                if self._is_cancelled():
                    print("\n[!ğŸ’¥] Analysis cancelled by user")
                    yield "<Cancelled>Analysis cancelled by user"
                    break

                # æå–å†…å®¹
                chunk_content = None
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        chunk_content = delta.reasoning_content
                    elif hasattr(delta, 'content') and delta.content:
                        chunk_content = delta.content

                if chunk_content:
                    print(chunk_content, end="")
                    yield chunk_content

            print()  # æµç»“æŸåæ¢è¡Œ

        except openai.APITimeoutError as e:
            print(f"\n[!ğŸ’¥] Error: Request timed out: {e}")
            yield f"<RequestException>Request timed out: {e}"
        except openai.APIConnectionError as e:
            print(f"\n[!ğŸ’¥] Error: Connection failed: {e}")
            yield f"<RequestException>Connection failed: {e}"
        except openai.AuthenticationError as e:
            print(f"\n[!ğŸ’¥] Error: Authentication failed: {e}")
            yield f"<RequestException>Authentication failed: {e}"
        except Exception as e:
            print(f"\n[!ğŸ’¥] Error: {e}")
            traceback.print_exc()
            yield f"<RequestException>{str(e)}"


class AnthropicApi(LLMApi):
    """
    Anthropic Claude API å®ç°

    æ”¯æŒï¼š
    - Anthropic å®˜æ–¹ API (https://api.anthropic.com)
    - å…¼å®¹ Anthropic çš„ç¬¬ä¸‰æ–¹ API
    """

    async def call(self, request: LLMRequest) -> LLMResponse:
        """
        è°ƒç”¨ Anthropic APIï¼Œè¿”å›å®Œæ•´å“åº”
        """
        self._reset_cancel_state()

        try:
            import anthropic
        except ImportError:
            raise ImportError("anthropic package is required. Install with: pip install anthropic")

        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_args = {'api_key': self.api_key}
        if self.base_url:
            client_args['base_url'] = self.base_url

        client = anthropic.AsyncAnthropic(**client_args)

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": request.prompt}]

        try:
            # å‘é€è¯·æ±‚ï¼ˆéæµå¼ï¼‰
            response = await client.messages.create(
                model=self.model_name,
                max_tokens=request.max_tokens,
                messages=messages,
            )

            content = response.content[0].text
            return LLMResponse(content=content, raw_prompt=request.prompt)

        except anthropic.APIConnectionError as e:
            error_msg = f"<RequestException>Connection failed: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except anthropic.AuthenticationError as e:
            error_msg = f"<RequestException>Authentication failed: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except anthropic.RateLimitError as e:
            error_msg = f"<RequestException>Rate limit exceeded: {e}"
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)
        except Exception as e:
            error_msg = f"<RequestException>{str(e)}"
            traceback.print_exc()
            return LLMResponse(content=error_msg, raw_prompt=request.prompt)

    async def call_stream(self, request: LLMRequest) -> AsyncIterator[str]:
        """
        æµå¼è°ƒç”¨ Anthropic API

        å®æ—¶æ‰“å°å“åº”å†…å®¹ï¼Œå¹¶é€å—è¿”å›
        """
        self._reset_cancel_state()

        try:
            import anthropic
        except ImportError:
            raise ImportError("anthropic package is required. Install with: pip install anthropic")

        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_args = {'api_key': self.api_key}
        if self.base_url:
            client_args['base_url'] = self.base_url

        client = anthropic.AsyncAnthropic(**client_args)

        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": request.prompt}]

        try:
            # å‘é€æµå¼è¯·æ±‚
            async with client.messages.stream(
                model=self.model_name,
                max_tokens=request.max_tokens,
                messages=messages,
            ) as stream:
                async for text in stream.text_stream:
                    if self._is_cancelled():
                        print("\n[!ğŸ’¥] Analysis cancelled by user")
                        yield "<Cancelled>Analysis cancelled by user"
                        break

                    print(text, end="")
                    yield text

            print()  # æµç»“æŸåæ¢è¡Œ

        except anthropic.APIConnectionError as e:
            print(f"\n[!ğŸ’¥] Error: Connection failed: {e}")
            yield f"<RequestException>Connection failed: {e}"
        except anthropic.AuthenticationError as e:
            print(f"\n[!ğŸ’¥] Error: Authentication failed: {e}")
            yield f"<RequestException>Authentication failed: {e}"
        except anthropic.RateLimitError as e:
            print(f"\n[!ğŸ’¥] Error: Rate limit exceeded: {e}")
            yield f"<RequestException>Rate limit exceeded: {e}"
        except Exception as e:
            print(f"\n[!ğŸ’¥] Error: {e}")
            traceback.print_exc()
            yield f"<RequestException>{str(e)}"


class LLMApiFactory:
    """
    LLM API å·¥å‚ç±»

    æ ¹æ®é…ç½®åˆ›å»ºåˆé€‚çš„ API å®ä¾‹
    """

    @staticmethod
    def _is_anthropic_api(base_url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º Anthropic API"""
        if not base_url:
            return False
        return 'anthropic' in base_url.lower()

    @staticmethod
    def create_from_config(settings: dict) -> LLMApi:
        """
        æ ¹æ®é…ç½®å­—å…¸åˆ›å»º API å®ä¾‹

        Args:
            settings: é…ç½®å­—å…¸ï¼Œåº”åŒ…å« base_url, api_key, model_name

        Returns:
            é…ç½®å¥½çš„ LLMApi å®ä¾‹
        """
        base_url = settings.get('base_url', '')
        api_key = settings.get('api_key', '')
        model_name = settings.get('model_name', 'gpt-4o')

        # æ£€æµ‹ API ç±»å‹
        if LLMApiFactory._is_anthropic_api(base_url):
            return AnthropicApi(base_url, api_key, model_name)
        else:
            return OpenAIApi(base_url, api_key, model_name)

    @staticmethod
    def create_openai(base_url: str = '', api_key: str = '', model_name: str = 'gpt-4o') -> OpenAIApi:
        """åˆ›å»º OpenAI API å®ä¾‹"""
        return OpenAIApi(base_url, api_key, model_name)

    @staticmethod
    def create_anthropic(base_url: str = 'https://api.anthropic.com',
                        api_key: str = '', model_name: str = 'claude-3-opus-20240229') -> AnthropicApi:
        """åˆ›å»º Anthropic API å®ä¾‹"""
        return AnthropicApi(base_url, api_key, model_name)
