"""
LLM API - ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹ API è°ƒç”¨æ¥å£

æ­¤æ¨¡å—æä¾›ç»Ÿä¸€çš„ LLM API è°ƒç”¨æŠ½è±¡ï¼Œæ”¯æŒå¤šç§æä¾›å•†ï¼š
- OpenAI å…¼å®¹ API
- Anthropic Claude API
- Mock å®ç°ï¼ˆç”¨äºè°ƒè¯•ï¼‰

ä½¿ç”¨ Provider æ¨¡å¼ï¼Œæ˜“äºæ‰©å±•æ–°çš„ API æä¾›å•†ã€‚
"""

import asyncio
import traceback
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Optional

import openai
import anthropic

from config import settings_manager, PROMPT_TEMPLATE
from task_guides import TASK_GUIDES, TASK_OUTPUT_FORMATS, get_mock_response


# ============================================================================
# æ•°æ®ç±»
# ============================================================================

@dataclass
class LLMRequest:
    """LLM è¯·æ±‚"""
    prompt: str
    task_tag: str
    max_tokens: int
    timeout: int = 600
    template_name: str = "general"


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    raw_prompt: str
    is_error: bool = False
    error_type: str = ""
    error_details: dict = field(default_factory=dict)


# ============================================================================
# æŠ½è±¡æ¥å£
# ============================================================================

class LLMProvider(ABC):
    """LLM æä¾›å•†æŠ½è±¡æ¥å£"""

    @abstractmethod
    async def call(self, request: LLMRequest) -> LLMResponse:
        """
        è°ƒç”¨ LLM API

        Args:
            request: LLM è¯·æ±‚å¯¹è±¡

        Returns:
            LLMResponse: å“åº”å¯¹è±¡
        """
        pass

    @abstractmethod
    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        pass


# ============================================================================
# æç¤ºè¯æ ¼å¼åŒ–å·¥å…·
# ============================================================================

class PromptFormatter:
    """æç¤ºè¯æ ¼å¼åŒ–å·¥å…·"""

    @staticmethod
    def format(request: LLMRequest) -> tuple[str, str]:
        """
        æ ¼å¼åŒ–æç¤ºè¯

        Args:
            request: LLM è¯·æ±‚å¯¹è±¡

        Returns:
            (formatted_prompt, prompt_for_feedback)
        """
        template_name = request.template_name or settings_manager.settings.get('prompt_template', 'general')
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}")

        # æ ¹æ®æ¨¡æ¿åç§°å†³å®šæ˜¯å¦ä½¿ç”¨æŒ‡å—
        if template_name.endswith("_wo_guide"):
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(request.task_tag, "")) \
                                                 .replace("{input}", request.prompt)
        else:
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(request.task_tag, "")) \
                                                 .replace("{guide}", TASK_GUIDES.get(request.task_tag, "")) \
                                                 .replace("{input}", request.prompt)

        prompt_for_feedback = formatted_prompt

        # è°ƒè¯•æ¨¡å¼æ‰“å°
        if settings_manager.settings.get('debug_mode', False):
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))

        return formatted_prompt, prompt_for_feedback


# ============================================================================
# OpenAI å…¼å®¹ API å®ç°
# ============================================================================

class OpenAIProvider(LLMProvider):
    """OpenAI å…¼å®¹ API æä¾›å•†"""

    def __init__(self):
        self._current_completion = None
        self._cancelled = False

    async def call(self, request: LLMRequest) -> LLMResponse:
        """è°ƒç”¨ OpenAI å…¼å®¹ API"""
        self._cancelled = False

        # æ ¼å¼åŒ–æç¤ºè¯
        formatted_prompt, prompt_for_feedback = PromptFormatter.format(request)

        # è·å–é…ç½®
        base_url = settings_manager.settings.get('base_url', '')
        api_key = settings_manager.settings.get('api_key', 'sk-none')
        model_name = settings_manager.settings.get('model_name', 'gpt-3.5-turbo')

        # è°ƒè¯•ä¿¡æ¯
        if settings_manager.settings.get('debug_mode', False):
            print(f"[ğŸ”—] OpenAIProvider: model={model_name}, timeout={request.timeout}s")
            print(f"[ğŸ”—] OpenAIProvider: sending {len(formatted_prompt)} chars")

        try:
            # æ„å»ºå®¢æˆ·ç«¯
            client_args = {'api_key': api_key}
            if base_url:
                client_args['base_url'] = base_url

            client = openai.AsyncOpenAI(**client_args)

            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": formatted_prompt}]

            # å‘é€è¯·æ±‚
            self._current_completion = await client.chat.completions.create(
                model=model_name,
                temperature=0.6,
                stream=True,
                max_tokens=request.max_tokens,
                messages=messages,
                timeout=float(request.timeout)
            )

            # å¤„ç†æµå¼å“åº”
            response_parts = []
            async for chunk in self._current_completion:
                if self._cancelled:
                    print("\n[!ğŸ’¥] OpenAIProvider: Cancelled by user")
                    await self._current_completion.close()
                    self._current_completion = None
                    return LLMResponse(
                        content="<Cancelled>Analysis cancelled by user",
                        raw_prompt=prompt_for_feedback,
                        is_error=False
                    )

                # æå–å†…å®¹
                chunk_content = None
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        chunk_content = delta.reasoning_content
                    elif hasattr(delta, 'content') and delta.content:
                        chunk_content = delta.content

                if chunk_content:
                    print(chunk_content, end="")
                    response_parts.append(chunk_content)

            print()  # æ¢è¡Œ
            self._current_completion = None
            final_response = "".join(response_parts)

            return LLMResponse(
                content=final_response,
                raw_prompt=prompt_for_feedback,
                is_error=False
            )

        except openai.APITimeoutError as e:
            print(f"[!ğŸ’¥] OpenAIProvider: Timeout - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Request timed out: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="APITimeoutError"
            )
        except openai.APIConnectionError as e:
            print(f"[!ğŸ’¥] OpenAIProvider: Connection failed - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Connection failed: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="APIConnectionError"
            )
        except openai.AuthenticationError as e:
            print(f"[!ğŸ’¥] OpenAIProvider: Authentication failed - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Authentication failed: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="AuthenticationError"
            )
        except Exception as e:
            print(f"[!ğŸ’¥] OpenAIProvider: Error - {e}")
            traceback.print_exc()
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>{str(e)}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type=Exception.__name__
            )

    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        self._cancelled = True


# ============================================================================
# Anthropic API å®ç°
# ============================================================================

class AnthropicProvider(LLMProvider):
    """Anthropic Claude API æä¾›å•†"""

    def __init__(self):
        self._current_completion = None
        self._cancelled = False

    async def call(self, request: LLMRequest) -> LLMResponse:
        """è°ƒç”¨ Anthropic API"""
        self._cancelled = False

        # æ ¼å¼åŒ–æç¤ºè¯
        formatted_prompt, prompt_for_feedback = PromptFormatter.format(request)

        # è·å–é…ç½®
        base_url = settings_manager.settings.get('base_url', '')
        api_key = settings_manager.settings.get('api_key', '')
        model_name = settings_manager.settings.get('model_name', 'claude-3-opus-20240229')

        try:
            # æ„å»ºå®¢æˆ·ç«¯
            client_args = {'api_key': api_key}
            if base_url:
                client_args['base_url'] = base_url

            client = anthropic.AsyncAnthropic(**client_args)

            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": formatted_prompt}]

            # å‘é€æµå¼è¯·æ±‚
            response_parts = []

            async with client.messages.stream(
                model=model_name,
                max_tokens=request.max_tokens,
                messages=messages,
            ) as stream:
                self._current_completion = stream
                async for text in stream.text_stream:
                    if self._cancelled:
                        print("\n[!ğŸ’¥] AnthropicProvider: Cancelled by user")
                        self._current_completion = None
                        return LLMResponse(
                            content="<Cancelled>Analysis cancelled by user",
                            raw_prompt=prompt_for_feedback,
                            is_error=False
                        )

                    print(text, end="")
                    response_parts.append(text)

            print()  # æ¢è¡Œ
            self._current_completion = None
            final_response = "".join(response_parts)

            return LLMResponse(
                content=final_response,
                raw_prompt=prompt_for_feedback,
                is_error=False
            )

        except anthropic.APIConnectionError as e:
            print(f"[!ğŸ’¥] AnthropicProvider: Connection failed - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Connection failed: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="APIConnectionError"
            )
        except anthropic.AuthenticationError as e:
            print(f"[!ğŸ’¥] AnthropicProvider: Authentication failed - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Authentication failed: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="AuthenticationError"
            )
        except anthropic.RateLimitError as e:
            print(f"[!ğŸ’¥] AnthropicProvider: Rate limit exceeded - {e}")
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>Rate limit exceeded: {e}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type="RateLimitError"
            )
        except Exception as e:
            print(f"[!ğŸ’¥] AnthropicProvider: Error - {e}")
            traceback.print_exc()
            self._current_completion = None
            return LLMResponse(
                content=f"<RequestException>{str(e)}",
                raw_prompt=prompt_for_feedback,
                is_error=True,
                error_type=Exception.__name__
            )

    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        self._cancelled = True


# ============================================================================
# Mock å®ç°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
# ============================================================================

class MockProvider(LLMProvider):
    """Mock æä¾›å•†ï¼Œç”¨äºè°ƒè¯•"""

    def __init__(self):
        self._cancelled = False

    async def call(self, request: LLMRequest) -> LLMResponse:
        """æ¨¡æ‹Ÿè°ƒç”¨ LLM"""
        self._cancelled = False

        # æ ¼å¼åŒ–æç¤ºè¯
        formatted_prompt, prompt_for_feedback = PromptFormatter.format(request)

        # è°ƒè¯•ä¿¡æ¯
        if settings_manager.settings.get('debug_mode', False):
            base_url = settings_manager.settings.get('base_url', 'N/A')
            api_key = settings_manager.settings.get('api_key', 'N/A')[:5] + "..."
            model_name = settings_manager.settings.get('model_name', 'mock_model')
            print(f"[DEBUGğŸ›] MockProvider: base_url={base_url}, api_key={api_key}")
            print(f"[DEBUGğŸ›] MockProvider: model={model_name}, timeout={request.timeout}s")
            print(f"[DEBUGğŸ›] MockProvider: sending {len(formatted_prompt)} chars")

        # è·å– mock å“åº”
        mock_response_full = get_mock_response(request.task_tag)

        print(f"[DEBUGğŸ›] Mock response for {request.task_tag}:")
        response_parts = []

        for line in mock_response_full.split('\n'):
            if self._cancelled:
                print("\n[!ğŸ’¥] MockProvider: Cancelled by user")
                return LLMResponse(
                    content="<Cancelled>Analysis cancelled by user",
                    raw_prompt=prompt_for_feedback,
                    is_error=False
                )

            print(f"[DEBUGğŸ›] {line}")
            response_parts.append(line)
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ

        return LLMResponse(
            content="\n".join(response_parts),
            raw_prompt=prompt_for_feedback,
            is_error=False
        )

    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        self._cancelled = True


# ============================================================================
# å·¥å‚ç±»
# ============================================================================

class LLMProviderFactory:
    """LLM æä¾›å•†å·¥å‚"""

    @staticmethod
    def create(base_url: str = None, use_mock: bool = False) -> LLMProvider:
        """
        åˆ›å»º LLM æä¾›å•†å®ä¾‹

        Args:
            base_url: API åŸºç¡€ URL
            use_mock: æ˜¯å¦ä½¿ç”¨ Mock æä¾›å•†

        Returns:
            LLMProvider: æä¾›å•†å®ä¾‹
        """
        if use_mock or settings_manager.settings.get('debug_mode', False):
            return MockProvider()

        if not base_url:
            base_url = settings_manager.settings.get('base_url', '')

        # æ£€æµ‹æ˜¯å¦ä¸º Anthropic API
        if 'anthropic' in base_url.lower():
            return AnthropicProvider()

        # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹ API
        return OpenAIProvider()


# ============================================================================
# ç»Ÿä¸€å®¢æˆ·ç«¯
# ============================================================================

class LLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯"""

    def __init__(self, provider: LLMProvider = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            provider: LLM æä¾›å•†ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨åˆ›å»º
        """
        self.provider = provider or LLMProviderFactory.create()

    async def call(self, request: LLMRequest) -> LLMResponse:
        """
        è°ƒç”¨ LLM API

        Args:
            request: LLM è¯·æ±‚å¯¹è±¡

        Returns:
            LLMResponse: å“åº”å¯¹è±¡
        """
        return await self.provider.call(request)

    def cancel(self):
        """å–æ¶ˆå½“å‰è¯·æ±‚"""
        self.provider.cancel()
